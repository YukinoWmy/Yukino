{"/Yukino/pages/about/": {
    "title": "About",
    "keywords": "Jekyll",
    "url": "/Yukino/pages/about/",
    "body": "This is an about page."
  },"/Yukino/pages/contact/": {
    "title": "Contact",
    "keywords": "Jekyll",
    "url": "/Yukino/pages/contact/",
    "body": "This is an contact page."
  },"/Yukino/pages/design/draft/": {
    "title": "Design Draft",
    "keywords": "Jekyll",
    "url": "/Yukino/pages/design/draft/",
    "body": "This is an draft page."
  },"/Yukino/jekyll/2024-03-24-Transformer-Based_Deep_Survival_Analysis.html": {
    "title": "Transformer-Based Deep Survival Analysis",
    "keywords": "Jekyll",
    "url": "/Yukino/jekyll/2024-03-24-Transformer-Based_Deep_Survival_Analysis.html",
    "body": "主要思想 使用序数回归来优化生存概率的估计 套用Transformer模型 方法 Transformer模型 1.核心概念 自注意力机制（Self-Attention） 自注意力机制是Transformer模型的核心，它允许输入序列中的每个元素同时考察序列中的其他所有元素，以计算自己的表示。这种机制使得模型能够捕获数据内部的复杂关系，尤其是长距离依赖关系。 多头注意力（Multi-Head Attention） 多头注意力机制是对自注意力机制的扩展。它将注意力分为多个“头”，每个头从不同的表示子空间学习输入之间的关系。这样可以使模型在多个子空间上并行学习，捕获数据的不同特征，从而提高模型的表现力。 2.Transformer架构 Transformer模型主要由编码器（Encoder）和解码器（Decoder）组成，每个部分包含多个相同的层，每层都有多头注意力和前馈神经网络（Feed-Forward Neural Network）。 编码器由一系列相同的层堆叠而成，每层包含两个主要部分：多头自注意力机制允许模型在编码时，同时考虑输入序列中的所有位置，以便捕获其中的全局依赖关系；前馈神经网络中每个位置都会应用同样的前馈神经网络，但是对于不同位置，它们是独立的，这有助于模型学习更加复杂的表示。 解码器同样由多层构成，每层包括三个部分：多头自注意力机制与编码器中的自注意力类似，但在解码器中，为了防止位置向前看，需要对输出进行遮蔽（Masking）；编码器-解码器注意力机制允许解码器的每个位置关注到编码器的所有位置，这对于序列到序列的任务特别重要，如机器翻译；前馈神经网络与编码器中的前馈网络相同。 3.位置编码（Positional Encoding） 由于Transformer模型不像RNN和LSTM那样具有递归结构，无法自然地处理序列的顺序信息，因此引入了位置编码来给每个位置的输入元素加入一定的位置信息。位置编码与输入向量相加，使得模型能够根据序列中元素的位置来学习它们的关系。 思考 这篇论文中将生存分析建模为序列的形式很有启发性，但将这些序列直接套用Transformer模型而没有对模型进行特色的优化调整有失妥当，生存分析与自然语言处理之间的差距还是比较大的，这样处理应该会丢失很多潜在维度的信息。同时论文中也提到了未来会在更大的数据集和更复杂的场景下测试，对于Transformer的改进我认为是必要的。"
  },"/Yukino/jekyll/2024-03-24-Deep_Survival_Analysis.html": {
    "title": "Deep Survival Analysis",
    "keywords": "Jekyll",
    "url": "/Yukino/jekyll/2024-03-24-Deep_Survival_Analysis.html",
    "body": "主要思想 以事件为中心的对齐方式 异构数据类型处理 分层生成方法（复杂系统建模） 方法 深度指数族（Deep Exponential Families, DEF） 深度指数族（DEF）是构建多层概率模型的一个框架，使用指数族分布作为构建模型的基础。DEF通过多层的潜在变量来捕捉数据之间的复杂依赖性。每一层的潜在变量都依赖于上一层的变量，形成了一个分层的结构。这种方法在模拟复杂数据结构（如EHR数据）中的潜在关系方面具有潜力，尤其是在处理高维度和稀疏数据时。通过这种方式，DEF能够为每个观测提供一个丰富的潜在表示，从而捕捉协变量和生存时间之间的复杂关系。 特别适合处理具有高维度和复杂结构的数据。 1.框架 一个指数族分布可以表示为： \\(p(x | \\theta) = h(x) \\exp(\\theta^\\top T(x) - A(\\theta))\\) $x$ 是观察数据，$\\theta$ 是自然参数，$T(x)$ 是充分统计量，$A(\\theta)$ 是对数配分函数，用于确保分布归一化，$h(x)$ 是基测度。 在深度指数族模型中，每一层的潜在变量都基于指数族分布，其参数由上一层的潜在变量决定。假设模型有 $L$ 层，第 $l$ 层的潜在变量表示为 $z^{(l)}$，则每一层的生成过程可以描述为以下形式： 对于第 $l$ 层，其中 $l = 1, \\ldots, L$， \\[z_j^{(l)} \\sim \\text{ExpFamily}(\\theta_j^{(l)})\\] \\(\\theta_j^{(l)} = g(W^{(l)} z^{(l-1)} + b^{(l)})\\) 其中$z_j^{(l)}$ 是第 $l$ 层的第 $j$ 个潜在变量，$\\theta_j^{(l)}$ 是控制 $z_j^{(l)}$ 分布的参数，$W^{(l)}$ 和 $b^{(l)}$ 分别是第 $l$ 层的权重和偏置，$g(\\cdot)$ 是一个非线性激活函数，如sigmoid函数或ReLU函数，第 $0$ 层 $z^{(0)}$ 对应于观察数据$x$。 在最顶层（$L$ 层），模型使用潜在变量 $z^{(L)}$ 来生成观察数据 $x$： \\(x \\sim \\text{ExpFamily}(\\eta(z^{(L)}))\\) 其中，$\\eta(\\cdot)$ 是一个函数，用于将 $L$ 层的潜在变量映射到观察数据的自然参数空间。 深度指数族模型的训练通常涉及到使用变分推断（Variational Inference, VI来近似后验分布，通过最大化证据下界（ELBO, Evidence Lower BOund来优化模型参数 $W^{(l)}$, $b^{(l)}$ 和变分参数。 2.举例 作为深度指数族的一个具体实例，深度主题模型通过堆叠多层主题表示来捕捉文档数据的层次语义结构。在这种模型中，最底层的潜在变量代表文档的主题分布，而上层的潜在变量则表示主题的更抽象和广泛的概念。通过这种多层次的主题表示，深度主题模型能够发现文档集合中更加细致和广泛的语义关系。 变分推断算法 为了在大规模EHR数据集上有效地训练深度生存分析模型，论文中使用了一种可扩展的变分推断算法。变分推断是一种近似推断方法，通过优化一个参数化的分布来近似真实的后验分布。在深度生存分析模型中，使用变分推断可以大大减少计算资源的需求，使得模型能够在合理的时间内完成训练。通过这种方法，即使是在包含数十万患者记录的大型数据集上，也能有效地估计模型参数。 1.基本思想 变分推断的目标是最小化真实后验分布与变分分布之间的差异。这种差异通常使用Kullback-Leibler（KL）散度来度量。通过最小化KL散度，变分推断寻找一个最佳的变分分布，使其尽可能接近真实的后验分布。 2.数学表述 给定观测数据$D$ 和模型参数$\\theta$，变分推断关注后验分布$P(\\theta | D)$。引入变分分布$q(\\theta; \\lambda)$，目标是找到最优的$\\lambda$ 以最小化KL散度： \\(KL(q(\\theta; \\lambda) \\,||\\, P(\\theta | D)) = \\int q(\\theta; \\lambda) \\log\\left(\\frac{q(\\theta; \\lambda)}{P(\\theta | D)}\\right) d\\theta\\) 由于直接最小化KL散度通常不可行，变分推断转而最大化证据下界（ELBO）： \\(\\text{ELBO}(\\lambda) = \\mathbb{E}_q[\\log P(D, \\theta)] - \\mathbb{E}_q[\\log q(\\theta; \\lambda)]\\) 其中$\\mathbb{E}_q[\\cdot]$ 表示在变分分布$q$下的期望值，$\\log P(D, \\theta)$ 是联合对数似然，包括数据的对数似然和参数的对数先验。 思考 这篇论文算是较早的一篇关于深度深层分析的论文。虽然应用在医疗领域，但核心我认为仍是在复杂系统的情境下，传统建模方式无力对系统精确描述的情况下转向深度学习或者机器学习寻求帮助。 问题的核心应该是对于系统复杂关系的简化与描述，同时结合医疗领域的特色，核心问题还应当包括对于不完整数据以及删失数据的处理与特征提取。对于这两个问题，文章主要以采用深指数族的方式解决，由于我对于深指数族的了解并不深刻，因此并不妄下判断，但其中对于潜在变量的定义与处理私以为与量子力学中关于隐变量的理论形式有着异曲同工之妙。"
  }}
