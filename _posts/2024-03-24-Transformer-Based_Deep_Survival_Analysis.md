---
title: Transformer-Based Deep Survival Analysis
author: Yukino
date: 2024-03-24
category: paper
layout: post
mermaid: true
---

## 主要思想
- 使用序数回归来优化生存概率的估计
- 套用Transformer模型

## 方法
### Transformer模型
#### 1.核心概念

自注意力机制（Self-Attention）
自注意力机制是Transformer模型的核心，它允许输入序列中的每个元素同时考察序列中的其他所有元素，以计算自己的表示。这种机制使得模型能够捕获数据内部的复杂关系，尤其是长距离依赖关系。

多头注意力（Multi-Head Attention）
多头注意力机制是对自注意力机制的扩展。它将注意力分为多个“头”，每个头从不同的表示子空间学习输入之间的关系。这样可以使模型在多个子空间上并行学习，捕获数据的不同特征，从而提高模型的表现力。

#### 2.Transformer架构

Transformer模型主要由编码器（Encoder）和解码器（Decoder）组成，每个部分包含多个相同的层，每层都有多头注意力和前馈神经网络（Feed-Forward Neural Network）。

编码器由一系列相同的层堆叠而成，每层包含两个主要部分：多头自注意力机制允许模型在编码时，同时考虑输入序列中的所有位置，以便捕获其中的全局依赖关系；前馈神经网络中每个位置都会应用同样的前馈神经网络，但是对于不同位置，它们是独立的，这有助于模型学习更加复杂的表示。

解码器同样由多层构成，每层包括三个部分：多头自注意力机制与编码器中的自注意力类似，但在解码器中，为了防止位置向前看，需要对输出进行遮蔽（Masking）；编码器-解码器注意力机制允许解码器的每个位置关注到编码器的所有位置，这对于序列到序列的任务特别重要，如机器翻译；前馈神经网络与编码器中的前馈网络相同。

#### 3.位置编码（Positional Encoding）

由于Transformer模型不像RNN和LSTM那样具有递归结构，无法自然地处理序列的顺序信息，因此引入了位置编码来给每个位置的输入元素加入一定的位置信息。位置编码与输入向量相加，使得模型能够根据序列中元素的位置来学习它们的关系。

## 思考

这篇论文中将生存分析建模为序列的形式很有启发性，但将这些序列直接套用Transformer模型而没有对模型进行特色的优化调整有失妥当，生存分析与自然语言处理之间的差距还是比较大的，这样处理应该会丢失很多潜在维度的信息。同时论文中也提到了未来会在更大的数据集和更复杂的场景下测试，对于Transformer的改进我认为是必要的。